{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starbelly.shell import *\n",
    "import pickle\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "When a crawl is paused, some of the in-memory state of the crawl is discarded, such as the set of URLs that have been previously seen. If the crawl is later resumed, then this state needs to be recomputed. The current implementation of the frontier reloads the set by iterating over all `crawl_item` and `frontier` documents for the given job and extracting the `url_hash` field. This notebook demonstrates the problematic performance of this implementation and explores some alternatives.\n",
    "\n",
    "# Loading from crawl_item\n",
    "\n",
    "The set of seen URLs is partially loaded from the `crawl_item` table. This section benchmarks the current implementation against a new implementation using a real crawl with 14k downloaded items and 164k items in the frontier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "async def get_item_url_hashes_old():\n",
    "    url_hashes = set()\n",
    "    query = (\n",
    "        r.table('crawl_item')\n",
    "        .between(('d06fedd2-c2a8-4ed6-9e39-94ed4036b254',r.minval),\n",
    "                 ('d06fedd2-c2a8-4ed6-9e39-94ed4036b254',r.maxval),\n",
    "                index='sync_index')\n",
    "        .pluck('url_hash')\n",
    "    )\n",
    "    async with db_pool.connection() as conn:\n",
    "        cursor = await query.run(conn)\n",
    "        async for item in starbelly.db.AsyncCursorIterator(cursor):\n",
    "            url_hashes.add(bytes(item['url_hash']))\n",
    "    return url_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.4 s, sys: 8.47 s, total: 31.9 s\n",
      "Wall time: 31.9 s\n"
     ]
    }
   ],
   "source": [
    "%time old_item_url_hashes = crun(get_item_url_hashes_old())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14223"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_item_url_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "async def get_item_url_hashes_new():\n",
    "    url_hashes = set()\n",
    "    row = 0\n",
    "    BATCH_SIZE = 1000\n",
    "    async with db_pool.connection() as conn:\n",
    "        while True:\n",
    "            query = (\n",
    "                r.table('crawl_item')\n",
    "                .between(('d06fedd2-c2a8-4ed6-9e39-94ed4036b254', row),\n",
    "                         ('d06fedd2-c2a8-4ed6-9e39-94ed4036b254', row + BATCH_SIZE),\n",
    "                         index='sync_index')\n",
    "                .map(lambda doc: [doc['url_hash']])\n",
    "                .reduce(lambda l, r: l.add(r))\n",
    "            )\n",
    "            try:\n",
    "                results = await query.run(conn)\n",
    "            except r.ReqlNonExistenceError:\n",
    "                break\n",
    "            url_hashes.update(bytes(r) for r in results)\n",
    "            row += BATCH_SIZE\n",
    "    return url_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 144 ms, sys: 16 ms, total: 160 ms\n",
      "Wall time: 239 ms\n"
     ]
    }
   ],
   "source": [
    "%time new_item_url_hashes = crun(get_item_url_hashes_new())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14223"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_item_url_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_item_url_hashes == old_item_url_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x83\\xe8\\xdb\\xb1y/\\xa1g\"\\xb6T\\x0f\"O\\x98\\x15'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_item_url_hashes.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x83\\xe8\\xdb\\xb1y/\\xa1g\"\\xb6T\\x0f\"O\\x98\\x15'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_item_url_hashes.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using map/reduce to eliminate key names and reduce the number of documents transferred is a major performance win: over 100x faster.\n",
    "\n",
    "# Loading from frontier\n",
    "\n",
    "The rest of the \"seen URLs\" set are loaded from the `frontier` table. Loading from frontier is trickier, because there's no index on this table that is suitable for fetching small batches. The index currently being used, `cost_index` is on the fields `(job_id, cost)`, and tens of thousands of items can have the same exact index value.\n",
    "\n",
    "Either a limit/offset approach needs to be used, or a else a new index must be added to include job_id and some secondary field that is more unique than cost. That secondary field could be `id`, but I can't think of any other use case for that index. A better secondary field might be `url` or `url_hash` since that might be useful for some other use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "async def get_frontier_url_hashes_old():\n",
    "    url_hashes = set()\n",
    "    query = (\n",
    "        r.table('frontier')\n",
    "        .between(('d06fedd2-c2a8-4ed6-9e39-94ed4036b254',r.minval),\n",
    "                 ('d06fedd2-c2a8-4ed6-9e39-94ed4036b254',r.maxval),\n",
    "                index='cost_index')\n",
    "        .pluck('url_hash')\n",
    "    )\n",
    "    async with db_pool.connection() as conn:\n",
    "        cursor = await query.run(conn)\n",
    "        async for item in starbelly.db.AsyncCursorIterator(cursor):\n",
    "            url_hashes.add(bytes(item['url_hash']))\n",
    "    return url_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing <Task pending coro=<ConnectionInstance._reader() running at /usr/local/lib/python3.6/dist-packages/rethinkdb/asyncio_net/net_asyncio.py:241> wait_for=<Future pending cb=[Task._wakeup()] created at /usr/lib/python3.6/asyncio/base_events.py:266> created at /usr/lib/python3.6/asyncio/tasks.py:540> took 0.233 seconds\n",
      "Executing <Task pending coro=<ConnectionInstance._reader() running at /usr/local/lib/python3.6/dist-packages/rethinkdb/asyncio_net/net_asyncio.py:241> wait_for=<Future pending cb=[Task._wakeup()] created at /usr/lib/python3.6/asyncio/base_events.py:266> created at /usr/lib/python3.6/asyncio/tasks.py:540> took 0.239 seconds\n",
      "Executing <Task pending coro=<ConnectionInstance._reader() running at /usr/local/lib/python3.6/dist-packages/rethinkdb/asyncio_net/net_asyncio.py:241> wait_for=<Future pending cb=[Task._wakeup()] created at /usr/lib/python3.6/asyncio/base_events.py:266> created at /usr/lib/python3.6/asyncio/tasks.py:540> took 0.203 seconds\n",
      "Executing <Task pending coro=<ConnectionInstance._reader() running at /usr/local/lib/python3.6/dist-packages/rethinkdb/asyncio_net/net_asyncio.py:241> wait_for=<Future pending cb=[Task._wakeup()] created at /usr/lib/python3.6/asyncio/base_events.py:266> created at /usr/lib/python3.6/asyncio/tasks.py:540> took 0.207 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 36s, sys: 1min 39s, total: 6min 16s\n",
      "Wall time: 6min 16s\n"
     ]
    }
   ],
   "source": [
    "%time old_frontier_url_hashes = crun(get_frontier_url_hashes_old())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch... not only does that take ages to run, the intense RethinkDB work locks up the event loop for long periods of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163739"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_frontier_url_hashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next example uses the same map/reduce as seen in `get_item_url_hashes_new()` and limit/skip instead of directly manipulating an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "async def get_frontier_url_hashes_limit():\n",
    "    url_hashes = set()\n",
    "    row = 0\n",
    "    BATCH_SIZE = 1000\n",
    "    async with db_pool.connection() as conn:\n",
    "        while True:\n",
    "            query = (\n",
    "                r.table('frontier')\n",
    "                .between(('d06fedd2-c2a8-4ed6-9e39-94ed4036b254',r.minval),\n",
    "                         ('d06fedd2-c2a8-4ed6-9e39-94ed4036b254',r.maxval),\n",
    "                        index='cost_index')\n",
    "                .skip(row)\n",
    "                .limit(BATCH_SIZE)\n",
    "                .map(lambda doc: [doc['url_hash']])\n",
    "                .reduce(lambda l, r: l.add(r))\n",
    "            )\n",
    "            try:\n",
    "                results = await query.run(conn)\n",
    "            except r.ReqlNonExistenceError:\n",
    "                break\n",
    "            url_hashes.update(bytes(r) for r in results)\n",
    "            row += BATCH_SIZE\n",
    "    return url_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.55 s, sys: 216 ms, total: 1.76 s\n",
      "Wall time: 37.1 s\n"
     ]
    }
   ],
   "source": [
    "%time new_frontier_url_hashes_limit = crun(get_frontier_url_hashes_limit())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the skip/offset approach is about 10x faster, but still feels very slow. I'll add an index on `(job_id, url_hash)` and try that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created': 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to create index.\n",
    "qrun(\n",
    "    r.table('frontier').index_create(\n",
    "        'reload_index', \n",
    "        [r.row[\"job_id\"], r.row[\"url_hash\"]]\n",
    "    ),\n",
    "    pool=super_db_pool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropped': 1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to drop index.\n",
    "qrun(\n",
    "    r.table('frontier').index_drop('reload_index'),\n",
    "    pool=super_db_pool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'\\x00\\x00\\x10n\\x9a\\n\\x85\\xdaq\\xa6\\xbd\\xa4\\xb45\\xc2\\xa9',\n",
       " b'\\x00\\x01!\\x05\\xf3E\\x84\\xfb\\xb2\\x1f\\x16&n\"\\xa9n',\n",
       " b'\\x00\\x01+VuE<\\x8bfoI\\x0cB?\\x195',\n",
       " b'\\x00\\x02n\\x80Z\\x8ebV!E\\x9aF\\xc7>\\xfb\\x05',\n",
       " b'\\x00\\x02\\xd3\\x8bqgO\\\\\\xb9\\x97\\x1aTt$W\\xfe',\n",
       " b'\\x00\\x03\\\\\\xdeB@\\xb7fV2\\x8f\\x89\\xd8~\\xc4\\xa2',\n",
       " b'\\x00\\x03t.4\\x14\\xd5\\xec:<m\\xff9\\xbe\\x14e',\n",
       " b'\\x00\\x03w\\x014\\xe2O\\xc0\\xba\\xec\\xaf\\x00\\x9a\\xb2\\x93\\xf5',\n",
       " b'\\x00\\x03{\\x0cj\\xf5|\\xe4gf\\x91s\\x9f\\xcf|\\xd8',\n",
       " b'\\x00\\x03\\x81M\\xe0\\x8c\\x01\\xe36C\\x84\\xc8\\x81\\xb9\\xad\\xdf',\n",
       " b'\\x00\\x03\\x8bb\\xe7h\\x14\\x14M\\xb6\\xe7^\\xfc\\xff\\xaf\\xfb',\n",
       " b\"\\x00\\x04'7\\x16K(\\x0e\\x9e\\x10\\xab\\x90\\x92\\xa63\\xc0\",\n",
       " b'\\x00\\x04oy\\xc2X\\xdf\\xd27\\xf5mi\\xb9/\\x9a\\xbb',\n",
       " b'\\x00\\x05+7\\x07\\xac\\xda\\x99\\x03E\\\\\\xb3o&<:',\n",
       " b'\\x00\\x05\\x8e%\\xab\\xcdn\\xa1\\xfd\\xa4\\xea\\xb4-\\x88\\xed\\xc8']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = qrun(\n",
    "   r.table('frontier')\n",
    "    .between(('d06fedd2-c2a8-4ed6-9e39-94ed4036b254',r.minval),\n",
    "             ('d06fedd2-c2a8-4ed6-9e39-94ed4036b254',r.maxval),\n",
    "            index='reload_index')\n",
    "    .order_by(index='reload_index')\n",
    "    .limit(15)\n",
    "    .map(lambda doc: [doc['url_hash']])\n",
    "    .reduce(lambda l, r: l.add(r))\n",
    ")\n",
    "\n",
    "display([bytes(r) for r in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_frontier_url_hashes_index():\n",
    "    url_hashes = set()\n",
    "    row = r.minval\n",
    "    BATCH_SIZE = 1000\n",
    "    async with db_pool.connection() as conn:\n",
    "        while True:\n",
    "            query = (\n",
    "                r.table('frontier')\n",
    "                .between(('d06fedd2-c2a8-4ed6-9e39-94ed4036b254',row),\n",
    "                         ('d06fedd2-c2a8-4ed6-9e39-94ed4036b254',r.maxval),\n",
    "                         left_bound='open',\n",
    "                         index='reload_index')\n",
    "                .order_by(index='reload_index')\n",
    "                .limit(BATCH_SIZE)\n",
    "                .map(lambda doc: [doc['url_hash']])\n",
    "                .reduce(lambda l, r: l.add(r))\n",
    "            )\n",
    "            try:\n",
    "                results = await query.run(conn)\n",
    "            except r.ReqlNonExistenceError:\n",
    "                break\n",
    "            url_hashes.update(bytes(r) for r in results)\n",
    "            row = results[-1]\n",
    "    return url_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 s, sys: 248 ms, total: 2.25 s\n",
      "Wall time: 8.76 s\n"
     ]
    }
   ],
   "source": [
    "%time new_frontier_url_hashes_index = crun(get_frontier_url_hashes_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163739"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_frontier_url_hashes_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xaeg A\\xcd\\xcbgj\\x18\\x06\\xd6\\x98\\x8f\\x02\\x1d\\x07'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_frontier_url_hashes_index.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This index method is about 4-5x faster than the skip/limit method, and is approaching a reasonable amount of time. But a frontier size of 160K may be on the low end of the scale.\n",
    "\n",
    "As a last resort, I'll try pickling the set, saving it into RethinkDB, then retrieving it and unpickling it. First we'll create a special table to hold the pickled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_changes': [{'new_val': {'db': 'starbelly',\n",
       "    'durability': 'hard',\n",
       "    'id': '6f962ab6-e065-4d78-92b9-55cc9bfa1f5e',\n",
       "    'indexes': [],\n",
       "    'name': 'job_url_seen',\n",
       "    'primary_key': 'id',\n",
       "    'shards': [{'nonvoting_replicas': [],\n",
       "      'primary_replica': '6fbd1a0195e8_ej8',\n",
       "      'replicas': ['6fbd1a0195e8_ej8']}],\n",
       "    'write_acks': 'majority'},\n",
       "   'old_val': None}],\n",
       " 'tables_created': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table\n",
    "qrun(\n",
    "    r.table_create('job_url_seen'),\n",
    "    pool=super_db_pool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_changes': [{'new_val': None,\n",
       "   'old_val': {'db': 'starbelly',\n",
       "    'durability': 'hard',\n",
       "    'id': '7cbccc3a-dc43-4c9b-a62c-428b8be7dd5c',\n",
       "    'indexes': [],\n",
       "    'name': 'job_url_seen',\n",
       "    'primary_key': 'id',\n",
       "    'shards': [{'nonvoting_replicas': [],\n",
       "      'primary_replica': '6fbd1a0195e8_ej8',\n",
       "      'replicas': ['6fbd1a0195e8_ej8']}],\n",
       "    'write_acks': 'majority'}}],\n",
       " 'tables_dropped': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop table\n",
    "qrun(\n",
    "    r.table_drop('job_url_seen'),\n",
    "    pool=super_db_pool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deleted': 0,\n",
       " 'errors': 0,\n",
       " 'generated_keys': ['169ee2da-0e44-4dc8-a59a-81755b316a74'],\n",
       " 'inserted': 1,\n",
       " 'replaced': 0,\n",
       " 'skipped': 0,\n",
       " 'unchanged': 0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_pkl = pickle.dumps(new_frontier_url_hashes_index)\n",
    "qrun(\n",
    "    r.table('job_url_seen').insert({\n",
    "        'job_id': 'd06fedd2-c2a8-4ed6-9e39-94ed4036b254',\n",
    "        'pickle': set_pkl,\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 100 ms, sys: 28 ms, total: 128 ms\n",
      "Wall time: 201 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = qrun(\n",
    "    r.table('job_url_seen')\n",
    "     .filter({'job_id': 'd06fedd2-c2a8-4ed6-9e39-94ed4036b254'})\n",
    "     .nth(0)\n",
    ")\n",
    "set_2_pkl = pickle.loads(result['pickle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163738"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set_2_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: one item is missing from the set because I called `new_frontier_url_hashes_index.pop().pop()` above.\n",
    "\n",
    "Conclusion: pickling is very fast, although it represents a significant change to the implementation. It runs the risk of inconsistency (e.g. the pickled set not matching the contents of `crawl_item` and `frontier` tables), but given that Starbelly goes to great lengths to pause a crawl in an orderly fashion, this is an acceptable risk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
